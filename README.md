# LLM In-Context Learning as Approximating Solomonoff Induction

This repository contains code and results for investigating whether large language models (LLMs) exhibit behavior consistent with Solomonoff induction when performing in-context learning on algorithmic sequences.

## Overview

We evaluate Qwen 2.5 models (0.5B, 3B, 7B, 32B parameters) on binary sequences generated by BF programs (a Universal Turing Machine implementation). By measuring predictive log-loss as more bits are revealed, we test whether:

1. **Models show decreasing per-bit loss** as they learn patterns in-context
2. **Models exhibit algorithmic simplicity bias** (lower regret on shorter programs), consistent with Solomonoff induction

## Key Results

The main findings are visualized in two plots:

1. **Token Loss Over Position**: Shows how average predictive log-loss decreases as more bits are revealed, demonstrating in-context learning.
2. **Regret vs Complexity**: Shows cumulative regret as a function of program description length, testing for Solomonoff-style simplicity bias.

See `results/BF_qwen/plots/comparison/` for the final plots.

## Repository Structure

```
.
├── analysis/                      # Analysis scripts
│   ├── generate_BF_binary_data.py    # Generate BF sequences
│   ├── llm_BF_binary_qwen.py         # Evaluate LLMs on sequences
│   ├── plot_multiple_models_token_loss.py     # Plot loss over position
│   └── plot_multiple_models_regret.py         # Plot regret vs complexity
├── data/                         # Data generation utilities
│   └── utms.py                   # BF UTM implementation
├── results/                      # Experimental results
│   └── BF_qwen/         # Qwen model evaluation results
│       ├── individual_results/   # Per-model evaluation outputs (.npz)
│       └── plots/                 # Generated plots
│           └── comparison/       # Main result plots
└── README.md                     # This file
```

## Installation

```bash
pip install -r requirements.txt
```

Key dependencies:
- `torch` and `transformers` (for Qwen models)
- `numpy` and `matplotlib` (for analysis and plotting)
- `tqdm` (for progress bars)

## Usage

See `results/BF_qwen/README.md` for detailed usage instructions and examples.

## Citation

If you use this code, please cite:

```bibtex
@article{llm_solomonoff_2024,
  title={LLM In-Context Learning as Approximating Solomonoff Induction},
  author={Your Name},
  year={2024}
}
```

## License

See LICENSE file for details.
